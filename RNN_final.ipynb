{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM\n",
    "A basic LSTM written in tensorflow. We'll use this to learn patterns in the matrix we've generated, and then sample it to generate new musical matrices that can be translated back to .midi files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# define params\n",
    "\n",
    "# meta-parameters\n",
    "model_save_path = 'C:/Users/Emerson/Desktop/saved_models/'\n",
    "generated_matrix_save_path = 'C:/Users/Emerson/Documents/bigdata/midis/generated/matrices/'\n",
    "data_path = 'C:/Users/Emerson/Documents/bigdata/midis/processed/mastermatrix.npy'\n",
    "test_path = 'C:/Users/Emerson/Documents/bigdata/midis/processed/testmatrix.npy'\n",
    "display_step = 200\n",
    "\n",
    "# learning parameters\n",
    "learning_rate = 1e-4\n",
    "epochs = 120\n",
    "batch_size = 64\n",
    "n_steps = 64\n",
    "\n",
    "#network parameters\n",
    "n_hidden = 256\n",
    "n_dense = 174\n",
    "n_input = 174\n",
    "n_output = n_input\n",
    "\n",
    "# create variables\n",
    "x = tf.placeholder(\"float\", [None, n_steps, n_input])\n",
    "y = tf.placeholder(\"float\", [None, n_output])\n",
    "\n",
    "weights = {\n",
    "    'out' : tf.Variable(tf.truncated_normal([n_hidden, n_dense])),\n",
    "    'bias' : tf.Variable(tf.truncated_normal([n_dense]))\n",
    "}\n",
    "\n",
    "lstm_cell = tf.contrib.rnn.BasicLSTMCell(n_hidden, forget_bias = 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Build network ops\n",
    "\n",
    "x_unstacked = tf.unstack(x, n_steps, 1) # reshape data from (batch_size, n_steps, n_input) to (n_steps, batch_size, n_input)\n",
    "\n",
    "outputs, states = tf.contrib.rnn.static_rnn(lstm_cell, x_unstacked, dtype=tf.float32)\n",
    "\n",
    "pred = tf.matmul(outputs[-1], weights['out']) + weights['bias']\n",
    "\n",
    "cost = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits = pred, labels = y))\n",
    "optimizer = tf.train.RMSPropOptimizer(learning_rate = learning_rate).minimize(cost)\n",
    "\n",
    "# Evaluate model\n",
    "probas = tf.sigmoid(pred)\n",
    "accuracy = tf.reduce_mean(tf.reduce_min(tf.cast(tf.equal(tf.round(probas), y), tf.float32), axis=-1))\n",
    "\n",
    "# initialize variables\n",
    "\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def load_batches(path, batch_size, n_steps, n_input):\n",
    "    '''\n",
    "    Generator function that loads batches of size batch_size from a pickled numpy \n",
    "    array. Batches contain random sequences of length n_steps.\n",
    "    '''\n",
    "    master_matrix = np.load(path, mmap_mode = 'r', encoding = 'latin1') # load memory map of the pickled note matrix\n",
    "    seq_starts = np.arange(0, len(master_matrix) - n_steps, 3) # define sequence starts as every third time-step.\n",
    "    np.random.shuffle(seq_starts) # shuffle the sequence starts to randomize the contents of the batches\n",
    "    print(\"Loading {} sequences in {} batches.\".format(len(seq_starts), len(seq_starts)//batch_size))\n",
    "    batch_x = np.empty((batch_size, n_steps, n_input))\n",
    "    batch_y = np.empty((batch_size, n_input))\n",
    "    for i, start in enumerate(seq_starts):\n",
    "        batch_x[i % batch_size] = master_matrix[start:start+n_steps]\n",
    "        batch_y[i % batch_size] = master_matrix[start+n_steps]\n",
    "        if i % batch_size == 0 and i != 0:\n",
    "            yield batch_x, batch_y\n",
    "            batch_x = np.empty((batch_size, n_steps, n_input), dtype = float)\n",
    "            batch_y = np.empty((batch_size, n_input), dtype = float)\n",
    "    batch_x = np.delete(batch_x, np.s_[i % batch_size + 1:], 0) # trim empty rows off last, shorter batch\n",
    "    batch_y = np.delete(batch_y, np.s_[i % batch_size + 1:], 0)\n",
    "    yield batch_x, batch_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading 169027 sequences in 2641 batches.\n",
      "Epoch 113, Batch 0: Minibatch Loss = 0.037120, Training accuracy = 0.375000\n",
      "Epoch 113, Batch 200: Minibatch Loss = 0.048970, Training accuracy = 0.296875\n",
      "Epoch 113, Batch 400: Minibatch Loss = 0.047676, Training accuracy = 0.312500\n",
      "Epoch 113, Batch 600: Minibatch Loss = 0.045513, Training accuracy = 0.312500\n",
      "Epoch 113, Batch 800: Minibatch Loss = 0.039739, Training accuracy = 0.375000\n",
      "Epoch 113, Batch 1000: Minibatch Loss = 0.041781, Training accuracy = 0.406250\n",
      "Epoch 113, Batch 1200: Minibatch Loss = 0.045061, Training accuracy = 0.234375\n",
      "Epoch 113, Batch 1400: Minibatch Loss = 0.039670, Training accuracy = 0.296875\n",
      "Epoch 113, Batch 1600: Minibatch Loss = 0.035186, Training accuracy = 0.312500\n",
      "Epoch 113, Batch 1800: Minibatch Loss = 0.031006, Training accuracy = 0.406250\n",
      "Epoch 113, Batch 2000: Minibatch Loss = 0.041830, Training accuracy = 0.296875\n",
      "Epoch 113, Batch 2200: Minibatch Loss = 0.039193, Training accuracy = 0.328125\n",
      "Epoch 113, Batch 2400: Minibatch Loss = 0.050703, Training accuracy = 0.296875\n",
      "Epoch 113, Batch 2600: Minibatch Loss = 0.045628, Training accuracy = 0.250000\n",
      "Loading 18403 sequences in 287 batches.\n",
      "Validation accuracy: 0.2156916542703079\n",
      "Model saved.\n",
      "Loading 169027 sequences in 2641 batches.\n",
      "Epoch 114, Batch 0: Minibatch Loss = 0.044569, Training accuracy = 0.312500\n",
      "Epoch 114, Batch 200: Minibatch Loss = 0.041649, Training accuracy = 0.375000\n",
      "Epoch 114, Batch 400: Minibatch Loss = 0.039603, Training accuracy = 0.359375\n",
      "Epoch 114, Batch 600: Minibatch Loss = 0.043232, Training accuracy = 0.375000\n",
      "Epoch 114, Batch 800: Minibatch Loss = 0.042962, Training accuracy = 0.312500\n",
      "Epoch 114, Batch 1000: Minibatch Loss = 0.047228, Training accuracy = 0.281250\n",
      "Epoch 114, Batch 1200: Minibatch Loss = 0.044958, Training accuracy = 0.343750\n",
      "Epoch 114, Batch 1400: Minibatch Loss = 0.047763, Training accuracy = 0.265625\n",
      "Epoch 114, Batch 1600: Minibatch Loss = 0.042306, Training accuracy = 0.328125\n",
      "Epoch 114, Batch 1800: Minibatch Loss = 0.047255, Training accuracy = 0.281250\n",
      "Epoch 114, Batch 2000: Minibatch Loss = 0.041050, Training accuracy = 0.421875\n",
      "Epoch 114, Batch 2200: Minibatch Loss = 0.038606, Training accuracy = 0.359375\n",
      "Epoch 114, Batch 2400: Minibatch Loss = 0.034092, Training accuracy = 0.406250\n",
      "Epoch 114, Batch 2600: Minibatch Loss = 0.048225, Training accuracy = 0.234375\n",
      "Loading 18403 sequences in 287 batches.\n",
      "Validation accuracy: 0.21314174107586345\n",
      "Model saved.\n",
      "Loading 169027 sequences in 2641 batches.\n",
      "Epoch 115, Batch 0: Minibatch Loss = 0.045557, Training accuracy = 0.312500\n",
      "Epoch 115, Batch 200: Minibatch Loss = 0.034841, Training accuracy = 0.437500\n",
      "Epoch 115, Batch 400: Minibatch Loss = 0.037078, Training accuracy = 0.421875\n",
      "Epoch 115, Batch 600: Minibatch Loss = 0.041161, Training accuracy = 0.281250\n",
      "Epoch 115, Batch 800: Minibatch Loss = 0.045308, Training accuracy = 0.281250\n",
      "Epoch 115, Batch 1000: Minibatch Loss = 0.040753, Training accuracy = 0.359375\n",
      "Epoch 115, Batch 1200: Minibatch Loss = 0.044809, Training accuracy = 0.375000\n",
      "Epoch 115, Batch 1400: Minibatch Loss = 0.050198, Training accuracy = 0.250000\n",
      "Epoch 115, Batch 1600: Minibatch Loss = 0.047451, Training accuracy = 0.328125\n",
      "Epoch 115, Batch 1800: Minibatch Loss = 0.051724, Training accuracy = 0.250000\n",
      "Epoch 115, Batch 2000: Minibatch Loss = 0.044755, Training accuracy = 0.328125\n",
      "Epoch 115, Batch 2200: Minibatch Loss = 0.036758, Training accuracy = 0.359375\n",
      "Epoch 115, Batch 2400: Minibatch Loss = 0.044702, Training accuracy = 0.312500\n",
      "Epoch 115, Batch 2600: Minibatch Loss = 0.054166, Training accuracy = 0.218750\n",
      "Loading 18403 sequences in 287 batches.\n",
      "Validation accuracy: 0.21886625746265054\n",
      "Model saved.\n",
      "Loading 169027 sequences in 2641 batches.\n",
      "Epoch 116, Batch 0: Minibatch Loss = 0.044720, Training accuracy = 0.343750\n",
      "Epoch 116, Batch 200: Minibatch Loss = 0.049833, Training accuracy = 0.250000\n",
      "Epoch 116, Batch 400: Minibatch Loss = 0.049283, Training accuracy = 0.281250\n",
      "Epoch 116, Batch 600: Minibatch Loss = 0.038142, Training accuracy = 0.390625\n",
      "Epoch 116, Batch 800: Minibatch Loss = 0.043590, Training accuracy = 0.296875\n",
      "Epoch 116, Batch 1000: Minibatch Loss = 0.047463, Training accuracy = 0.312500\n",
      "Epoch 116, Batch 1200: Minibatch Loss = 0.048534, Training accuracy = 0.312500\n",
      "Epoch 116, Batch 1400: Minibatch Loss = 0.048743, Training accuracy = 0.328125\n",
      "Epoch 116, Batch 1600: Minibatch Loss = 0.039171, Training accuracy = 0.312500\n",
      "Epoch 116, Batch 1800: Minibatch Loss = 0.045906, Training accuracy = 0.218750\n",
      "Epoch 116, Batch 2000: Minibatch Loss = 0.049362, Training accuracy = 0.250000\n",
      "Epoch 116, Batch 2200: Minibatch Loss = 0.049811, Training accuracy = 0.265625\n",
      "Epoch 116, Batch 2400: Minibatch Loss = 0.037883, Training accuracy = 0.390625\n",
      "Epoch 116, Batch 2600: Minibatch Loss = 0.038635, Training accuracy = 0.234375\n",
      "Loading 18403 sequences in 287 batches.\n",
      "Validation accuracy: 0.20732886905574965\n",
      "Model saved.\n",
      "Loading 169027 sequences in 2641 batches.\n",
      "Epoch 117, Batch 0: Minibatch Loss = 0.047343, Training accuracy = 0.218750\n",
      "Epoch 117, Batch 200: Minibatch Loss = 0.034427, Training accuracy = 0.531250\n",
      "Epoch 117, Batch 400: Minibatch Loss = 0.045281, Training accuracy = 0.328125\n",
      "Epoch 117, Batch 600: Minibatch Loss = 0.050423, Training accuracy = 0.250000\n",
      "Epoch 117, Batch 800: Minibatch Loss = 0.042133, Training accuracy = 0.359375\n",
      "Epoch 117, Batch 1000: Minibatch Loss = 0.044640, Training accuracy = 0.312500\n",
      "Epoch 117, Batch 1200: Minibatch Loss = 0.041553, Training accuracy = 0.375000\n",
      "Epoch 117, Batch 1400: Minibatch Loss = 0.042898, Training accuracy = 0.343750\n",
      "Epoch 117, Batch 1600: Minibatch Loss = 0.042990, Training accuracy = 0.437500\n",
      "Epoch 117, Batch 1800: Minibatch Loss = 0.032003, Training accuracy = 0.468750\n",
      "Epoch 117, Batch 2000: Minibatch Loss = 0.051238, Training accuracy = 0.359375\n",
      "Epoch 117, Batch 2200: Minibatch Loss = 0.044394, Training accuracy = 0.312500\n",
      "Epoch 117, Batch 2400: Minibatch Loss = 0.039442, Training accuracy = 0.359375\n",
      "Epoch 117, Batch 2600: Minibatch Loss = 0.051912, Training accuracy = 0.218750\n",
      "Loading 18403 sequences in 287 batches.\n",
      "Validation accuracy: 0.20804966520518064\n",
      "Model saved.\n",
      "Loading 169027 sequences in 2641 batches.\n",
      "Epoch 118, Batch 0: Minibatch Loss = 0.054970, Training accuracy = 0.265625\n",
      "Epoch 118, Batch 200: Minibatch Loss = 0.048292, Training accuracy = 0.343750\n",
      "Epoch 118, Batch 400: Minibatch Loss = 0.041965, Training accuracy = 0.312500\n",
      "Epoch 118, Batch 600: Minibatch Loss = 0.045708, Training accuracy = 0.234375\n",
      "Epoch 118, Batch 800: Minibatch Loss = 0.042279, Training accuracy = 0.312500\n",
      "Epoch 118, Batch 1000: Minibatch Loss = 0.049747, Training accuracy = 0.312500\n",
      "Epoch 118, Batch 1200: Minibatch Loss = 0.035874, Training accuracy = 0.343750\n",
      "Epoch 118, Batch 1400: Minibatch Loss = 0.036996, Training accuracy = 0.406250\n",
      "Epoch 118, Batch 1600: Minibatch Loss = 0.044975, Training accuracy = 0.312500\n",
      "Epoch 118, Batch 1800: Minibatch Loss = 0.037156, Training accuracy = 0.390625\n",
      "Epoch 118, Batch 2000: Minibatch Loss = 0.038939, Training accuracy = 0.296875\n",
      "Epoch 118, Batch 2200: Minibatch Loss = 0.039278, Training accuracy = 0.390625\n",
      "Epoch 118, Batch 2400: Minibatch Loss = 0.034871, Training accuracy = 0.437500\n",
      "Epoch 118, Batch 2600: Minibatch Loss = 0.037204, Training accuracy = 0.296875\n",
      "Loading 18403 sequences in 287 batches.\n",
      "Validation accuracy: 0.21588231649042833\n",
      "Model saved.\n",
      "Loading 169027 sequences in 2641 batches.\n",
      "Epoch 119, Batch 0: Minibatch Loss = 0.054392, Training accuracy = 0.203125\n",
      "Epoch 119, Batch 200: Minibatch Loss = 0.044838, Training accuracy = 0.296875\n",
      "Epoch 119, Batch 400: Minibatch Loss = 0.044305, Training accuracy = 0.281250\n",
      "Epoch 119, Batch 600: Minibatch Loss = 0.055688, Training accuracy = 0.312500\n",
      "Epoch 119, Batch 800: Minibatch Loss = 0.030501, Training accuracy = 0.500000\n",
      "Epoch 119, Batch 1000: Minibatch Loss = 0.037816, Training accuracy = 0.359375\n",
      "Epoch 119, Batch 1200: Minibatch Loss = 0.041923, Training accuracy = 0.296875\n",
      "Epoch 119, Batch 1400: Minibatch Loss = 0.042583, Training accuracy = 0.281250\n",
      "Epoch 119, Batch 1600: Minibatch Loss = 0.052643, Training accuracy = 0.218750\n",
      "Epoch 119, Batch 1800: Minibatch Loss = 0.048901, Training accuracy = 0.296875\n",
      "Epoch 119, Batch 2000: Minibatch Loss = 0.044511, Training accuracy = 0.281250\n",
      "Epoch 119, Batch 2200: Minibatch Loss = 0.044190, Training accuracy = 0.359375\n",
      "Epoch 119, Batch 2400: Minibatch Loss = 0.035226, Training accuracy = 0.468750\n",
      "Epoch 119, Batch 2600: Minibatch Loss = 0.039643, Training accuracy = 0.343750\n",
      "Loading 18403 sequences in 287 batches.\n",
      "Validation accuracy: 0.20730096727816594\n",
      "Model saved.\n",
      "Loading 169027 sequences in 2641 batches.\n",
      "Epoch 120, Batch 0: Minibatch Loss = 0.046437, Training accuracy = 0.359375\n",
      "Epoch 120, Batch 200: Minibatch Loss = 0.050970, Training accuracy = 0.281250\n",
      "Epoch 120, Batch 400: Minibatch Loss = 0.046838, Training accuracy = 0.250000\n",
      "Epoch 120, Batch 600: Minibatch Loss = 0.042617, Training accuracy = 0.265625\n",
      "Epoch 120, Batch 800: Minibatch Loss = 0.053495, Training accuracy = 0.328125\n",
      "Epoch 120, Batch 1000: Minibatch Loss = 0.044719, Training accuracy = 0.281250\n",
      "Epoch 120, Batch 1200: Minibatch Loss = 0.032382, Training accuracy = 0.453125\n",
      "Epoch 120, Batch 1400: Minibatch Loss = 0.046532, Training accuracy = 0.359375\n",
      "Epoch 120, Batch 1600: Minibatch Loss = 0.036365, Training accuracy = 0.390625\n",
      "Epoch 120, Batch 1800: Minibatch Loss = 0.043962, Training accuracy = 0.312500\n",
      "Epoch 120, Batch 2000: Minibatch Loss = 0.036601, Training accuracy = 0.343750\n",
      "Epoch 120, Batch 2200: Minibatch Loss = 0.046731, Training accuracy = 0.328125\n",
      "Epoch 120, Batch 2400: Minibatch Loss = 0.034719, Training accuracy = 0.375000\n",
      "Epoch 120, Batch 2600: Minibatch Loss = 0.038855, Training accuracy = 0.328125\n",
      "Loading 18403 sequences in 287 batches.\n",
      "Validation accuracy: 0.2050858755182061\n",
      "Model saved.\n",
      "Optimization Finished!\n"
     ]
    }
   ],
   "source": [
    "# Launch the graph\n",
    "saver = tf.train.Saver(max_to_keep = 120)\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    ### Uncomment this line to restore a saved model and continue training. Leave commented out to start from scratch.\n",
    "    # saver.restore(sess, model_save_path + '112')\n",
    "    # Keep training for the prescribed number of epochs\n",
    "    e = 0 # if continuing from a checkpoint, set e to the number of the epoch you are starting at.\n",
    "    while e <= epochs:\n",
    "        batch = 0\n",
    "        for batch_x, batch_y in load_batches(data_path, batch_size, n_steps, n_input):\n",
    "            # Run optimization ops (backprop)\n",
    "            sess.run(optimizer, feed_dict={x: batch_x, y: batch_y})\n",
    "            if batch % display_step == 0:\n",
    "                # Calculate batch accuracy and loss\n",
    "                loss, train_acc = sess.run((cost, accuracy), feed_dict={x: batch_x, y: batch_y})\n",
    "                print(\"Epoch \" + str(e) + \", Batch \" + str(batch) + \n",
    "                      \": Minibatch Loss = {:.6f}, Training accuracy = {:.6f}\".format(loss, train_acc))\n",
    "            batch += 1\n",
    "        valid_acc = np.empty(0)\n",
    "        for batch_x, batch_y in load_batches(test_path, batch_size, n_steps, n_input):\n",
    "            batch_acc = sess.run(accuracy, feed_dict={x: batch_x, y: batch_y})\n",
    "            valid_acc = np.append(valid_acc, batch_acc)\n",
    "        print(\"Validation accuracy: {}\".format(valid_acc.mean()))\n",
    "        saver.save(sess, model_save_path + str(e))\n",
    "        print(\"Model saved.\")\n",
    "        e += 1\n",
    "    print(\"Optimization Finished!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample Generation\n",
    "Now that the network has been trained, we can randomly sample it and feed the output back in as input, creating a self-writing digital piano roll."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation complete!\n"
     ]
    }
   ],
   "source": [
    "sample_len = 256 # length in sixteenth-notes. a length of 256 generates a ~31 second sample.\n",
    "checkpoint_num = 10 # select a saved model to generate the sample from\n",
    "checkpoint_path = model_save_path + str(checkpoint_num) \n",
    "sample_num = 1\n",
    "seed = None\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    output_matrix = np.empty((sample_len, n_input))\n",
    "\n",
    "    # load checkpoint\n",
    "    loader = tf.train.Saver()\n",
    "    loader.restore(sess, checkpoint_path)\n",
    "    \n",
    "    # use a seed for generation if one is provided\n",
    "    if seed is None:\n",
    "        input_seq = np.zeros((1, n_steps, n_input))\n",
    "    else:\n",
    "        input_seq = np.array(seed).reshape((1, n_steps, n_input))\n",
    "    \n",
    "    for timestep in range(sample_len):\n",
    "        # get predicted probabilities for next timestep\n",
    "        probs = sess.run(probas, feed_dict = {x: input_seq})\n",
    "        output = np.zeros((1, 1, n_input))\n",
    "        # randomly sample using predicted probabilities\n",
    "        for i, p in enumerate(probs[0]):\n",
    "            output[0, 0, i] = 1 if np.random.random() < p else 0\n",
    "        # ensure consistency of the output\n",
    "        for i, n in enumerate(output[0,0,:]):\n",
    "            if i % 2 == 1: # check notes being held first\n",
    "                # if a note was not sounding last time-step and is not being played, \n",
    "                # it cannot be sounding in the current timestep.\n",
    "                if input_seq[0, -1, i] == 0 and output[0, 0, i - 1] == 0:\n",
    "                    output[0, 0, i] = 0\n",
    "            # check notes being played this time step\n",
    "            if i % 2 == 0:\n",
    "                if n == 1: # if a note is being played,\n",
    "                    output[0, 0, i + 1] = 1 # it must also be sounding.\n",
    "        # save the output for this timestep to the output matrix\n",
    "        output_matrix[timestep] = output\n",
    "        # prepare the next input sequence\n",
    "        input_seq = np.append(input_seq[:, 1: ,:], output, axis = 1)\n",
    "        \n",
    "    print(\"Generation complete!\")\n",
    "    np.save(generated_matrix_save_path + 'epoch ' + str(checkpoint_num) + ' sample ' + str(sample_num), output_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
